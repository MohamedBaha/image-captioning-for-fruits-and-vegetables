{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ljHQdFR7EOvo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.contrib import keras\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "L = keras.layers\n",
        "K = keras.backend\n",
        "import utils\n",
        "import time\n",
        "import zipfile\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import random\n",
        "from random import choice\n",
        "import os\n",
        "#from keras_utils import reset_tf_session\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fQ7bMdcFHVUB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50c6c9e0-4d28-43b6-9b4b-05f1109c17ac"
      },
      "cell_type": "code",
      "source": [
        "# Leave USE_GOOGLE_DRIVE = False if you're running locally!\n",
        "# We recommend to set USE_GOOGLE_DRIVE = True in Google Colab!\n",
        "# If set to True, we will mount Google Drive, so that you can restore your checkpoint \n",
        "# and continue trainig even if your previous Colab session dies.\n",
        "# If set to True, follow on-screen instructions to access Google Drive (you must have a Google account).\n",
        "USE_GOOGLE_DRIVE = False\n",
        "\n",
        "def mount_google_drive():\n",
        "    from google.colab import drive\n",
        "    mount_directory = \"/content/gdrive\"\n",
        "    drive.mount(mount_directory)\n",
        "    drive_root = mount_directory + \"/\" + list(filter(lambda x: x[0] != '.', os.listdir(mount_directory)))[0] + \"/colab\"\n",
        "    return drive_root\n",
        "\n",
        "CHECKPOINT_ROOT = \"\"\n",
        "if USE_GOOGLE_DRIVE:\n",
        "    CHECKPOINT_ROOT = mount_google_drive() + \"/\"\n",
        "\n",
        "def get_checkpoint_path(epoch=None):\n",
        "    if epoch is None:\n",
        "        return os.path.abspath(CHECKPOINT_ROOT + \"weights\")\n",
        "    else:\n",
        "        return os.path.abspath(CHECKPOINT_ROOT + \"weights_{}\".format(epoch))\n",
        "      \n",
        "# example of checkpoint dir\n",
        "print(get_checkpoint_path(10))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/weights_10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wBjDuMSFJvi4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 299"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zqtGR9BQJ3RZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# we take the last hidden layer of IncetionV3 as an image embedding\n",
        "def get_cnn_encoder():\n",
        "    K.set_learning_phase(False)\n",
        "    model = keras.applications.InceptionV3(include_top=False)\n",
        "    preprocess_for_model = keras.applications.inception_v3.preprocess_input\n",
        "\n",
        "    model = keras.models.Model(model.inputs, keras.layers.GlobalAveragePooling2D()(model.output))\n",
        "    return model, preprocess_for_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0mBhVwViKACB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "4a4f0138-7333-48ec-c9c1-6e2e8bd70b02"
      },
      "cell_type": "code",
      "source": [
        "# load prepared embeddings\n",
        "train_img_embeds = utils.read_pickle(\"train_img_embeds.pickle\")\n",
        "train_img_fns = utils.read_pickle(\"train_img_fns.pickle\")\n",
        "val_img_embeds = utils.read_pickle(\"val_img_embeds.pickle\")\n",
        "val_img_fns = utils.read_pickle(\"val_img_fns.pickle\")\n",
        "# check shapes\n",
        "print(train_img_embeds.shape, len(train_img_fns))\n",
        "print(val_img_embeds.shape, len(val_img_fns))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-9b8f9a3e8d9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_img_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_img_embeds.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_img_fns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_img_fns.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mval_img_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val_img_embeds.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_img_fns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val_img_fns.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# check shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'utils' has no attribute 'read_pickle'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "5-0gPR9nKCzS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# check prepared samples of images\n",
        "list(filter(lambda x: x.endswith(\"_sample.zip\"), os.listdir(\".\")))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FTfC6bbtKEsL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# extract captions from zip\n",
        "def get_captions_for_fns(fns, zip_fn, zip_json_path):\n",
        "    zf = zipfile.ZipFile(zip_fn)\n",
        "    j = json.loads(zf.read(zip_json_path).decode(\"utf8\"))\n",
        "    id_to_fn = {img[\"id\"]: img[\"file_name\"] for img in j[\"images\"]}\n",
        "    fn_to_caps = defaultdict(list)\n",
        "    for cap in j['annotations']:\n",
        "        fn_to_caps[id_to_fn[cap['image_id']]].append(cap['caption'])\n",
        "    fn_to_caps = dict(fn_to_caps)\n",
        "    return list(map(lambda x: fn_to_caps[x], fns))\n",
        "    \n",
        "train_captions = get_captions_for_fns(train_img_fns, \"captions_train-val2014.zip\", \n",
        "                                      \"annotations/captions_train2014.json\")\n",
        "\n",
        "val_captions = get_captions_for_fns(val_img_fns, \"captions_train-val2014.zip\", \n",
        "                                      \"annotations/captions_val2014.json\")\n",
        "\n",
        "# check shape\n",
        "print(len(train_img_fns), len(train_captions))\n",
        "print(len(val_img_fns), len(val_captions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uqz16cxiKGoD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# look at training example (each has 5 captions)\n",
        "def show_trainig_example(train_img_fns, train_captions, example_idx=0):\n",
        "    \"\"\"\n",
        "    You can change example_idx and see different images\n",
        "    \"\"\"\n",
        "    zf = zipfile.ZipFile(\"train2014_sample.zip\")\n",
        "    captions_by_file = dict(zip(train_img_fns, train_captions))\n",
        "    all_files = set(train_img_fns)\n",
        "    found_files = list(filter(lambda x: x.filename.rsplit(\"/\")[-1] in all_files, zf.filelist))\n",
        "    example = found_files[example_idx]\n",
        "    img = utils.decode_image_from_buf(zf.read(example))\n",
        "    plt.imshow(utils.image_center_crop(img))\n",
        "    plt.title(\"\\n\".join(captions_by_file[example.filename.rsplit(\"/\")[-1]]))\n",
        "    plt.show()\n",
        "    \n",
        "show_trainig_example(train_img_fns, train_captions, example_idx=142)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uAhhIb7JKHaJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# preview captions data\n",
        "train_captions[:2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gZgcW3ZlKI77",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# special tokens\n",
        "PAD = \"#PAD#\"\n",
        "UNK = \"#UNK#\"\n",
        "START = \"#START#\"\n",
        "END = \"#END#\"\n",
        "\n",
        "# split sentence into tokens (split into lowercased words)\n",
        "def split_sentence(sentence):\n",
        "    return list(filter(lambda x: len(x) > 0, re.split('\\W+', sentence.lower())))\n",
        "\n",
        "def generate_vocabulary(train_captions):\n",
        "    \"\"\"\n",
        "    Return {token: index} for all train tokens (words) that occur 5 times or more, \n",
        "        `index` should be from 0 to N, where N is a number of unique tokens in the resulting dictionary.\n",
        "    Use `split_sentence` function to split sentence into tokens.\n",
        "    Also, add PAD (for batch padding), UNK (unknown, out of vocabulary), \n",
        "        START (start of sentence) and END (end of sentence) tokens into the vocabulary.\n",
        "    \"\"\"\n",
        "    vocab = ### YOUR CODE HERE ###\n",
        "    return {token: index for index, token in enumerate(sorted(vocab))}\n",
        "    \n",
        "def caption_tokens_to_indices(captions, vocab):\n",
        "    \"\"\"\n",
        "    `captions` argument is an array of arrays:\n",
        "    [\n",
        "        [\n",
        "            \"image1 caption1\",\n",
        "            \"image1 caption2\",\n",
        "            ...\n",
        "        ],\n",
        "        [\n",
        "            \"image2 caption1\",\n",
        "            \"image2 caption2\",\n",
        "            ...\n",
        "        ],\n",
        "        ...\n",
        "    ]\n",
        "    Use `split_sentence` function to split sentence into tokens.\n",
        "    Replace all tokens with vocabulary indices, use UNK for unknown words (out of vocabulary).\n",
        "    Add START and END tokens to start and end of each sentence respectively.\n",
        "    For the example above you should produce the following:\n",
        "    [\n",
        "        [\n",
        "            [vocab[START], vocab[\"image1\"], vocab[\"caption1\"], vocab[END]],\n",
        "            [vocab[START], vocab[\"image1\"], vocab[\"caption2\"], vocab[END]],\n",
        "            ...\n",
        "        ],\n",
        "        ...\n",
        "    ]\n",
        "    \"\"\"\n",
        "    res = ### YOUR CODE HERE ###\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FexSPz2pKLJ7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# prepare vocabulary\n",
        "vocab = generate_vocabulary(train_captions)\n",
        "vocab_inverse = {idx: w for w, idx in vocab.items()}\n",
        "print(len(vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fb5-IR-qKNED",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# replace tokens with indices\n",
        "train_captions_indexed = caption_tokens_to_indices(train_captions, vocab)\n",
        "val_captions_indexed = caption_tokens_to_indices(val_captions, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "31WykJfPKOsb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# we will use this during training\n",
        "def batch_captions_to_matrix(batch_captions, pad_idx, max_len=None):\n",
        "    \"\"\"\n",
        "    `batch_captions` is an array of arrays:\n",
        "    [\n",
        "        [vocab[START], ..., vocab[END]],\n",
        "        [vocab[START], ..., vocab[END]],\n",
        "        ...\n",
        "    ]\n",
        "    Put vocabulary indexed captions into np.array of shape (len(batch_captions), columns),\n",
        "        where \"columns\" is max(map(len, batch_captions)) when max_len is None\n",
        "        and \"columns\" = min(max_len, max(map(len, batch_captions))) otherwise.\n",
        "    Add padding with pad_idx where necessary.\n",
        "    Input example: [[1, 2, 3], [4, 5]]\n",
        "    Output example: np.array([[1, 2, 3], [4, 5, pad_idx]]) if max_len=None\n",
        "    Output example: np.array([[1, 2], [4, 5]]) if max_len=2\n",
        "    Output example: np.array([[1, 2, 3], [4, 5, pad_idx]]) if max_len=100\n",
        "    Try to use numpy, we need this function to be fast!\n",
        "    \"\"\"\n",
        "    matrix = ###YOUR CODE HERE###\n",
        "    return matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dZQVUj53KQf7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# make sure you use correct argument in caption_tokens_to_indices\n",
        "assert len(caption_tokens_to_indices(train_captions[:10], vocab)) == 10\n",
        "assert len(caption_tokens_to_indices(train_captions[:5], vocab)) == 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R1CTZunbKSy6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class final_model:\n",
        "    # CNN encoder\n",
        "    encoder, preprocess_for_model = get_cnn_encoder()\n",
        "    saver.restore(s, get_checkpoint_path())  # keras applications corrupt our graph, so we restore trained weights\n",
        "    \n",
        "    # containers for current lstm state\n",
        "    lstm_c = tf.Variable(tf.zeros([1, LSTM_UNITS]), name=\"cell\")\n",
        "    lstm_h = tf.Variable(tf.zeros([1, LSTM_UNITS]), name=\"hidden\")\n",
        "\n",
        "    # input images\n",
        "    input_images = tf.placeholder('float32', [1, IMG_SIZE, IMG_SIZE, 3], name='images')\n",
        "\n",
        "    # get image embeddings\n",
        "    img_embeds = encoder(input_images)\n",
        "\n",
        "    # initialize lstm state conditioned on image\n",
        "    init_c = init_h = decoder.img_embed_bottleneck_to_h0(decoder.img_embed_to_bottleneck(img_embeds))\n",
        "    init_lstm = tf.assign(lstm_c, init_c), tf.assign(lstm_h, init_h)\n",
        "    \n",
        "    # current word index\n",
        "    current_word = tf.placeholder('int32', [1], name='current_input')\n",
        "\n",
        "    # embedding for current word\n",
        "    word_embed = decoder.word_embed(current_word)\n",
        "\n",
        "    # apply lstm cell, get new lstm states\n",
        "    new_c, new_h = decoder.lstm(word_embed, tf.nn.rnn_cell.LSTMStateTuple(lstm_c, lstm_h))[1]\n",
        "\n",
        "    # compute logits for next token\n",
        "    new_logits = decoder.token_logits(decoder.token_logits_bottleneck(new_h))\n",
        "    # compute probabilities for next token\n",
        "    new_probs = tf.nn.softmax(new_logits)\n",
        "\n",
        "    # `one_step` outputs probabilities of next token and updates lstm hidden state\n",
        "    one_step = new_probs, tf.assign(lstm_c, new_c), tf.assign(lstm_h, new_h)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LISnYb3lKVZM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# look at how temperature works for probability distributions\n",
        "# for high temperature we have more uniform distribution\n",
        "_ = np.array([0.5, 0.4, 0.1])\n",
        "for t in [0.01, 0.1, 1, 10, 100]:\n",
        "    print(\" \".join(map(str, _**(1/t) / np.sum(_**(1/t)))), \"with temperature\", t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YIVhnxM5KYT6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# this is an actual prediction loop\n",
        "def generate_caption(image, t=1, sample=False, max_len=20):\n",
        "    \"\"\"\n",
        "    Generate caption for given image.\n",
        "    if `sample` is True, we will sample next token from predicted probability distribution.\n",
        "    `t` is a temperature during that sampling,\n",
        "        higher `t` causes more uniform-like distribution = more chaos.\n",
        "    \"\"\"\n",
        "    # condition lstm on the image\n",
        "    s.run(final_model.init_lstm, \n",
        "          {final_model.input_images: [image]})\n",
        "    \n",
        "    # current caption\n",
        "    # start with only START token\n",
        "    caption = [vocab[START]]\n",
        "    \n",
        "    for _ in range(max_len):\n",
        "        next_word_probs = s.run(final_model.one_step, \n",
        "                                {final_model.current_word: [caption[-1]]})[0]\n",
        "        next_word_probs = next_word_probs.ravel()\n",
        "        \n",
        "        # apply temperature\n",
        "        next_word_probs = next_word_probs**(1/t) / np.sum(next_word_probs**(1/t))\n",
        "\n",
        "        if sample:\n",
        "            next_word = np.random.choice(range(len(vocab)), p=next_word_probs)\n",
        "        else:\n",
        "            next_word = np.argmax(next_word_probs)\n",
        "\n",
        "        caption.append(next_word)\n",
        "        if next_word == vocab[END]:\n",
        "            break\n",
        "       \n",
        "    return list(map(vocab_inverse.get, caption))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Tfpv7XTKaj7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# look at validation prediction example\n",
        "def apply_model_to_image_raw_bytes(raw):\n",
        "    img = utils.decode_image_from_buf(raw)\n",
        "    fig = plt.figure(figsize=(7, 7))\n",
        "    plt.grid('off')\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img)\n",
        "    img = utils.crop_and_preprocess(img, (IMG_SIZE, IMG_SIZE), final_model.preprocess_for_model)\n",
        "    print(' '.join(generate_caption(img)[1:-1]))\n",
        "    plt.show()\n",
        "\n",
        "def show_valid_example(val_img_fns, example_idx=0):\n",
        "    zf = zipfile.ZipFile(\"val2014_sample.zip\")\n",
        "    all_files = set(val_img_fns)\n",
        "    found_files = list(filter(lambda x: x.filename.rsplit(\"/\")[-1] in all_files, zf.filelist))\n",
        "    example = found_files[example_idx]\n",
        "    apply_model_to_image_raw_bytes(zf.read(example))\n",
        "    \n",
        "show_valid_example(val_img_fns, example_idx=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tizS6DgOKeZT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# sample more images from validation\n",
        "for idx in np.random.choice(range(len(zipfile.ZipFile(\"val2014_sample.zip\").filelist) - 1), 10):\n",
        "    show_valid_example(val_img_fns, example_idx=idx)\n",
        "    time.sleep(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iSGY5ygkKhBV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "download_utils.download_file(\n",
        "    \"http://www.bijouxandbits.com/wp-content/uploads/2016/06/portal-cake-10.jpg\",\n",
        "    \"portal-cake-10.jpg\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "St_4KoBlKi30",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "apply_model_to_image_raw_bytes(open(\"portal-cake-10.jpg\", \"rb\").read())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
